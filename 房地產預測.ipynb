{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57c69326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>sector</th>\n",
       "      <th>year</th>\n",
       "      <th>amount_new_house_transactions</th>\n",
       "      <th>year_end_registered_population_10k</th>\n",
       "      <th>total_households_10k</th>\n",
       "      <th>year_end_resident_population_10k</th>\n",
       "      <th>year_end_total_employed_population_10k</th>\n",
       "      <th>year_end_urban_non_private_employees_10k</th>\n",
       "      <th>private_individual_and_other_employees_10k</th>\n",
       "      <th>...</th>\n",
       "      <th>medical_health_rehabilitation_institution_dense</th>\n",
       "      <th>medical_health_first_aid_center_dense</th>\n",
       "      <th>medical_health_blood_donation_station_dense</th>\n",
       "      <th>medical_health_disease_prevention_institution_dense</th>\n",
       "      <th>medical_health_general_hospital_dense</th>\n",
       "      <th>medical_health_clinic_dense</th>\n",
       "      <th>education_training_school_education_middle_school_dense</th>\n",
       "      <th>education_training_school_education_primary_school_dense</th>\n",
       "      <th>education_training_school_education_kindergarten_dense</th>\n",
       "      <th>education_training_school_education_research_institution_dense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>sector 1</td>\n",
       "      <td>2019</td>\n",
       "      <td>13827.14</td>\n",
       "      <td>953.72</td>\n",
       "      <td>313.8455</td>\n",
       "      <td>1530.59</td>\n",
       "      <td>1125.89</td>\n",
       "      <td>400.22</td>\n",
       "      <td>725.672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.600000e-07</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>sector 1</td>\n",
       "      <td>2019</td>\n",
       "      <td>13827.14</td>\n",
       "      <td>953.72</td>\n",
       "      <td>313.8455</td>\n",
       "      <td>1530.59</td>\n",
       "      <td>1125.89</td>\n",
       "      <td>400.22</td>\n",
       "      <td>725.672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.600000e-07</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>sector 1</td>\n",
       "      <td>2019</td>\n",
       "      <td>13827.14</td>\n",
       "      <td>953.72</td>\n",
       "      <td>313.8455</td>\n",
       "      <td>1530.59</td>\n",
       "      <td>1125.89</td>\n",
       "      <td>400.22</td>\n",
       "      <td>725.672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.600000e-07</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>sector 1</td>\n",
       "      <td>2019</td>\n",
       "      <td>13827.14</td>\n",
       "      <td>953.72</td>\n",
       "      <td>313.8455</td>\n",
       "      <td>1530.59</td>\n",
       "      <td>1125.89</td>\n",
       "      <td>400.22</td>\n",
       "      <td>725.672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.600000e-07</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>sector 1</td>\n",
       "      <td>2019</td>\n",
       "      <td>13827.14</td>\n",
       "      <td>953.72</td>\n",
       "      <td>313.8455</td>\n",
       "      <td>1530.59</td>\n",
       "      <td>1125.89</td>\n",
       "      <td>400.22</td>\n",
       "      <td>725.672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.600000e-07</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        month    sector  year  amount_new_house_transactions  \\\n",
       "0  2019-01-01  sector 1  2019                       13827.14   \n",
       "1  2019-01-01  sector 1  2019                       13827.14   \n",
       "2  2019-01-01  sector 1  2019                       13827.14   \n",
       "3  2019-01-01  sector 1  2019                       13827.14   \n",
       "4  2019-01-01  sector 1  2019                       13827.14   \n",
       "\n",
       "   year_end_registered_population_10k  total_households_10k  \\\n",
       "0                              953.72              313.8455   \n",
       "1                              953.72              313.8455   \n",
       "2                              953.72              313.8455   \n",
       "3                              953.72              313.8455   \n",
       "4                              953.72              313.8455   \n",
       "\n",
       "   year_end_resident_population_10k  year_end_total_employed_population_10k  \\\n",
       "0                           1530.59                                 1125.89   \n",
       "1                           1530.59                                 1125.89   \n",
       "2                           1530.59                                 1125.89   \n",
       "3                           1530.59                                 1125.89   \n",
       "4                           1530.59                                 1125.89   \n",
       "\n",
       "   year_end_urban_non_private_employees_10k  \\\n",
       "0                                    400.22   \n",
       "1                                    400.22   \n",
       "2                                    400.22   \n",
       "3                                    400.22   \n",
       "4                                    400.22   \n",
       "\n",
       "   private_individual_and_other_employees_10k  ...  \\\n",
       "0                                     725.672  ...   \n",
       "1                                     725.672  ...   \n",
       "2                                     725.672  ...   \n",
       "3                                     725.672  ...   \n",
       "4                                     725.672  ...   \n",
       "\n",
       "   medical_health_rehabilitation_institution_dense  \\\n",
       "0                                         0.000113   \n",
       "1                                         0.000113   \n",
       "2                                         0.000113   \n",
       "3                                         0.000113   \n",
       "4                                         0.000113   \n",
       "\n",
       "   medical_health_first_aid_center_dense  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   medical_health_blood_donation_station_dense  \\\n",
       "0                                          0.0   \n",
       "1                                          0.0   \n",
       "2                                          0.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "\n",
       "   medical_health_disease_prevention_institution_dense  \\\n",
       "0                                       8.600000e-07     \n",
       "1                                       8.600000e-07     \n",
       "2                                       8.600000e-07     \n",
       "3                                       8.600000e-07     \n",
       "4                                       8.600000e-07     \n",
       "\n",
       "   medical_health_general_hospital_dense  medical_health_clinic_dense  \\\n",
       "0                               0.000041                     0.000038   \n",
       "1                               0.000041                     0.000038   \n",
       "2                               0.000041                     0.000038   \n",
       "3                               0.000041                     0.000038   \n",
       "4                               0.000041                     0.000038   \n",
       "\n",
       "   education_training_school_education_middle_school_dense  \\\n",
       "0                                           0.000016         \n",
       "1                                           0.000016         \n",
       "2                                           0.000016         \n",
       "3                                           0.000016         \n",
       "4                                           0.000016         \n",
       "\n",
       "   education_training_school_education_primary_school_dense  \\\n",
       "0                                           0.000028          \n",
       "1                                           0.000028          \n",
       "2                                           0.000028          \n",
       "3                                           0.000028          \n",
       "4                                           0.000028          \n",
       "\n",
       "   education_training_school_education_kindergarten_dense  \\\n",
       "0                                           0.000063        \n",
       "1                                           0.000063        \n",
       "2                                           0.000063        \n",
       "3                                           0.000063        \n",
       "4                                           0.000063        \n",
       "\n",
       "   education_training_school_education_research_institution_dense  \n",
       "0                                           0.000014               \n",
       "1                                           0.000014               \n",
       "2                                           0.000014               \n",
       "3                                           0.000014               \n",
       "4                                           0.000014               \n",
       "\n",
       "[5 rows x 246 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('merged_training_table.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a04c1af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非數值欄位: ['month', 'sector', 'keyword', 'source']\n"
     ]
    }
   ],
   "source": [
    "X=df.drop('amount_new_house_transactions',axis=1)\n",
    "y=df['amount_new_house_transactions']\n",
    "\n",
    "non_numeric = X.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "print(\"非數值欄位:\", non_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6ac0225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['month'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b069cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = pd.to_datetime(df['month'])\n",
    "df['year'] = df['year'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaa74e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非數值欄位: ['month', 'sector', 'year', 'keyword', 'source']\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('amount_new_house_transactions', axis=1)  # 轉型後再建\n",
    "non_numeric = X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "print(\"非數值欄位:\", non_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4868a3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['month'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0618068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\user\\anaconda3\\lib\\site-packages (3.0.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from xgboost) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from xgboost) (1.15.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4e347fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear:CV (Kaggle two-stage score) mean: 0.0\n",
      "Linear:Each split: [0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF:CV (Kaggle two-stage score) mean: 0.0\n",
      "RF:Each split: [0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['total_fixed_asset_investment_10k']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB:CV (Kaggle two-stage score) mean: 0.0\n",
      "XGB:Each split: [0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23076\\71088798.py:85: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.abs(y_pred - y_true) / np.abs(y_true)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit , cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "\n",
    "num_cols=X.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_cols=X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "pre_linear=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy='median')),\n",
    "            ('scaler',StandardScaler())\n",
    "        ]),num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pre_tree=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy='median')),\n",
    "        ]),num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "linear=LinearRegression()\n",
    "pipe_linear=Pipeline([\n",
    "    ('pre_linear',pre_linear),\n",
    "    ('linear',linear),\n",
    "])\n",
    "\n",
    "rf=RandomForestRegressor(\n",
    "    n_estimators=100,        \n",
    "    max_depth=12,            \n",
    "    min_samples_split=20,    \n",
    "    min_samples_leaf=5,      \n",
    "    max_features='sqrt',   \n",
    "    bootstrap=True,          \n",
    "    max_samples=0.7,         \n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "pipe_rf=Pipeline([\n",
    "    ('pre_rf',pre_tree),\n",
    "    ('rf',rf),\n",
    "])\n",
    "\n",
    "xgb=XGBRegressor(\n",
    "    n_estimators=200,        \n",
    "    max_depth=6,             \n",
    "    learning_rate=0.1,      \n",
    "    subsample=0.8,           \n",
    "    colsample_bytree=0.8,    \n",
    "    min_child_weight=5,      \n",
    "    reg_lambda=1.0,          \n",
    "    n_jobs=-1,              \n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"            \n",
    ")\n",
    "pipe_xgb=Pipeline([\n",
    "    ('pre_xgb',pre_tree),\n",
    "    ('xgb',xgb),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#設定評分方法\n",
    "# —— 兩階段 MAPE：回傳「分數，越大越好」——\n",
    "def two_stage_mape_score(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    # APE：處理 y_true=0 的情況（與常識一致：真值為 0 且預測也為 0 → 0；否則 → inf）\n",
    "    ape = np.where(\n",
    "        y_true == 0,\n",
    "        np.where(np.isclose(y_pred, 0.0), 0.0, np.inf),\n",
    "        np.abs(y_pred - y_true) / np.abs(y_true)\n",
    "    )\n",
    "\n",
    "    n = len(ape)\n",
    "    frac_big = np.mean(ape > 1)  # >100% 的比例\n",
    "\n",
    "    # 第一階段：若 >30% 的樣本 APE>1 → 直接 0 分\n",
    "    if frac_big > 0.3:\n",
    "        return 0.0\n",
    "\n",
    "    # 第二階段：只用 APE<=1 的樣本，計算縮放後的 MAPE，score = 1 - scaled_mape\n",
    "    mask_ok = (ape <= 1) & np.isfinite(ape)\n",
    "    if mask_ok.sum() == 0:\n",
    "        return 0.0  # 理論上不會到這裡，保險起見\n",
    "\n",
    "    mape_ok = ape[mask_ok].mean()\n",
    "    frac_ok = mask_ok.sum() / n\n",
    "    scaled_mape = mape_ok / frac_ok\n",
    "    score = 1.0 - scaled_mape\n",
    "\n",
    "    # 分數界於 [0,1]（理論上已是如此，這裡穩健處理）\n",
    "    return float(np.clip(score, 0.0, 1.0))\n",
    "\n",
    "# 包成 sklearn scorer（越大越好）\n",
    "kaggle_scorer = make_scorer(two_stage_mape_score, greater_is_better=True)\n",
    "\n",
    "\n",
    "tscv=TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "cv_scores_linear=cross_val_score(\n",
    "    pipe_linear,X[num_cols],y,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"Linear:CV (Kaggle two-stage score) mean:\", cv_scores_linear.mean())\n",
    "print(\"Linear:Each split:\", cv_scores_linear)\n",
    "\n",
    "cv_scores_rf=cross_val_score(\n",
    "    pipe_rf,X[num_cols],y,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"RF:CV (Kaggle two-stage score) mean:\", cv_scores_rf.mean())\n",
    "print(\"RF:Each split:\", cv_scores_rf)\n",
    "\n",
    "cv_scores_xgb=cross_val_score(\n",
    "    pipe_xgb,X[num_cols],y,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"XGB:CV (Kaggle two-stage score) mean:\", cv_scores_xgb.mean())\n",
    "print(\"XGB:Each split:\", cv_scores_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "356d6e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "要丟掉的欄位數: 13\n"
     ]
    }
   ],
   "source": [
    "# 1. 全 NaN\n",
    "nan_cols = X.columns[X.isna().all()]\n",
    "\n",
    "# 2. 幾乎全 NaN（例如超過 95% 缺失）\n",
    "mostly_nan_cols = X.columns[X.isna().mean() > 0.95]\n",
    "\n",
    "# 3. 常數或接近常數欄\n",
    "const_cols = X.columns[X.nunique(dropna=True) <= 1]\n",
    "\n",
    "# 設定一個閾值，例如：非 NaN 比例 < 5% 就刪掉\n",
    "low_info_cols = X.columns[(X.notna().mean() < 0.05)]\n",
    "\n",
    "drop_cols = nan_cols.union(mostly_nan_cols).union(const_cols).union(low_info_cols)\n",
    "print(\"要丟掉的欄位數:\", len(drop_cols))\n",
    "\n",
    "X=X.drop(columns=drop_cols)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8aedd288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y==0 數量: 65160 / 450300\n"
     ]
    }
   ],
   "source": [
    "print(\"y==0 數量:\", (y == 0).sum(), \"/\", len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e15db682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear:CV (Kaggle two-stage score) mean: 0.0\n",
      "Linear:Each split: [0. 0. 0. 0. 0.]\n",
      "RF:CV (Kaggle two-stage score) mean: 0.0\n",
      "RF:Each split: [0. 0. 0. 0. 0.]\n",
      "XGB:CV (Kaggle two-stage score) mean: 0.0\n",
      "XGB:Each split: [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# ===== 修正版兩階段 MAPE (處理 y_true=0) =====\n",
    "def two_stage_mape_score(y_true, y_pred, epsilon=1e-6):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    # 避免除以零：分母加 epsilon\n",
    "    ape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
    "\n",
    "    n = len(ape)\n",
    "    frac_big = np.mean(ape > 1)  # >100% 的比例\n",
    "\n",
    "    # 第一階段：若超過 30% 的樣本 APE > 1 → 直接 0 分\n",
    "    if frac_big > 0.3:\n",
    "        return 0.0\n",
    "\n",
    "    # 第二階段：只算 APE <= 1 的樣本\n",
    "    mask_ok = (ape <= 1) & np.isfinite(ape)\n",
    "    if mask_ok.sum() == 0:\n",
    "        return 0.0\n",
    "\n",
    "    mape_ok = ape[mask_ok].mean()\n",
    "    frac_ok = mask_ok.sum() / n\n",
    "    scaled_mape = mape_ok / frac_ok\n",
    "    score = 1.0 - scaled_mape\n",
    "\n",
    "    return float(np.clip(score, 0.0, 1.0))\n",
    "\n",
    "# ===== 包成 sklearn scorer (越大越好) =====\n",
    "kaggle_scorer = make_scorer(two_stage_mape_score, greater_is_better=True)\n",
    "\n",
    "\n",
    "num_cols=X.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_cols=X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "pre_linear=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy='median')),\n",
    "            ('scaler',StandardScaler())\n",
    "        ]),num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pre_tree=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy='median')),\n",
    "        ]),num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "linear=LinearRegression()\n",
    "pipe_linear=Pipeline([\n",
    "    ('pre_linear',pre_linear),\n",
    "    ('linear',linear),\n",
    "])\n",
    "\n",
    "rf=RandomForestRegressor(\n",
    "    n_estimators=100,        \n",
    "    max_depth=12,            \n",
    "    min_samples_split=20,    \n",
    "    min_samples_leaf=5,      \n",
    "    max_features='sqrt',   \n",
    "    bootstrap=True,          \n",
    "    max_samples=0.7,         \n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "pipe_rf=Pipeline([\n",
    "    ('pre_rf',pre_tree),\n",
    "    ('rf',rf),\n",
    "])\n",
    "\n",
    "xgb=XGBRegressor(\n",
    "    n_estimators=200,        \n",
    "    max_depth=6,             \n",
    "    learning_rate=0.1,      \n",
    "    subsample=0.8,           \n",
    "    colsample_bytree=0.8,    \n",
    "    min_child_weight=5,      \n",
    "    reg_lambda=1.0,          \n",
    "    n_jobs=-1,              \n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"        \n",
    ")\n",
    "pipe_xgb=Pipeline([\n",
    "    ('pre_xgb',pre_tree),\n",
    "    ('xgb',xgb),\n",
    "])\n",
    "\n",
    "tscv=TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "cv_scores_linear=cross_val_score(\n",
    "    pipe_linear,X[num_cols],y,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"Linear:CV (Kaggle two-stage score) mean:\", cv_scores_linear.mean())\n",
    "print(\"Linear:Each split:\", cv_scores_linear)\n",
    "\n",
    "cv_scores_rf=cross_val_score(\n",
    "    pipe_rf,X[num_cols],y,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"RF:CV (Kaggle two-stage score) mean:\", cv_scores_rf.mean())\n",
    "print(\"RF:Each split:\", cv_scores_rf)\n",
    "\n",
    "cv_scores_xgb=cross_val_score(\n",
    "    pipe_xgb,X[num_cols],y,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"XGB:CV (Kaggle two-stage score) mean:\", cv_scores_xgb.mean())\n",
    "print(\"XGB:Each split:\", cv_scores_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37605d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各折 >100% 比例： [0.22371752165223185, 0.34900732844770155, 0.37534976682211857, 0.31491005996002663, 0.3866222518321119] 平均： 0.32992138574283814\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "# 假設：X(含 month, sector, y_col)、features、pipeline 都已經準備好\n",
    "\n",
    "def frac_big_ape(y_true, y_pred, eps=1e-6):\n",
    "    ape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), eps)\n",
    "    return float((ape > 1).mean())\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "ratios = []\n",
    "for tr_idx, te_idx in tscv.split(X):\n",
    "    Xtr, Xte = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "    ytr, yte = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "    mdl = pipe_linear.fit(Xtr[num_cols], ytr)\n",
    "    pred = mdl.predict(Xte[num_cols])\n",
    "    pred = np.clip(pred, 0.0, None)  # y≥0\n",
    "    ratios.append(frac_big_ape(yte.values, pred))\n",
    "\n",
    "print(\"各折 >100% 比例：\", ratios, \"平均：\", np.mean(ratios))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d19d2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各折 >100% 比例： [0.3487008660892738, 0.40924716855429716, 0.33557628247834775, 0.45169886742171883, 0.4340972684876749] 平均： 0.3958640906062625\n"
     ]
    }
   ],
   "source": [
    "def frac_big_ape(y_true, y_pred, eps=1e-6):\n",
    "    ape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), eps)\n",
    "    return float((ape > 1).mean())\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "ratios = []\n",
    "for tr_idx, te_idx in tscv.split(X):\n",
    "    Xtr, Xte = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "    ytr, yte = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "    mdl = pipe_rf.fit(Xtr[num_cols], ytr)\n",
    "    pred = mdl.predict(Xte[num_cols])\n",
    "    pred = np.clip(pred, 0.0, None)  # y≥0\n",
    "    ratios.append(frac_big_ape(yte.values, pred))\n",
    "\n",
    "print(\"各折 >100% 比例：\", ratios, \"平均：\", np.mean(ratios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fcdfbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各折 >100% 比例： [0.4108061292471685, 0.34739506995336444, 0.3012658227848101, 0.39949367088607596, 0.3853431045969354] 平均： 0.36886075949367086\n"
     ]
    }
   ],
   "source": [
    "def frac_big_ape(y_true, y_pred, eps=1e-6):\n",
    "    ape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), eps)\n",
    "    return float((ape > 1).mean())\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "ratios = []\n",
    "for tr_idx, te_idx in tscv.split(X):\n",
    "    Xtr, Xte = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "    ytr, yte = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "    mdl = pipe_xgb.fit(Xtr[num_cols], ytr)\n",
    "    pred = mdl.predict(Xte[num_cols])\n",
    "    pred = np.clip(pred, 0.0, None)  # y≥0\n",
    "    ratios.append(frac_big_ape(yte.values, pred))\n",
    "\n",
    "print(\"各折 >100% 比例：\", ratios, \"平均：\", np.mean(ratios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2e34ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy:CV (Kaggle two-stage score) mean: 0.0\n",
      "Dummy:Each split: [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy_reg=DummyRegressor(strategy='median')\n",
    "pipe_dummy_reg=Pipeline([\n",
    "    ('pre_dummy_reg',pre_tree),\n",
    "    ('dummy_reg',dummy_reg),\n",
    "])\n",
    "cv_scores_dummy_ref=cross_val_score(\n",
    "    pipe_dummy_reg,X[num_cols],y,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"Dummy:CV (Kaggle two-stage score) mean:\", cv_scores_dummy_ref.mean())\n",
    "print(\"Dummy:Each split:\", cv_scores_dummy_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52c35895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year_month\"] = pd.to_datetime(df[\"month\"]).dt.to_period(\"M\").dt.to_timestamp()\n",
    "df = df.sort_values([\"sector\", \"year_month\"]).reset_index(drop=True)\n",
    "\n",
    "X = df.drop(\"amount_new_house_transactions\", axis=1)\n",
    "y = df[\"amount_new_house_transactions\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23dffb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  baseline  two_stage_score  frac_APE>100%\n",
      "0     lag1         0.995663       0.002385\n",
      "2      ma3         0.989868       0.005241\n",
      "3      ma6         0.981318       0.009460\n",
      "1    lag12         0.945110       0.028621\n"
     ]
    }
   ],
   "source": [
    "date_col  = \"year_month\"   \n",
    "group_col = \"sector\"\n",
    "\n",
    "g = df.groupby(group_col)[\"amount_new_house_transactions\"]\n",
    "\n",
    "\n",
    "#建立天真基線\n",
    "preds = {\n",
    "    \"lag1\":  g.shift(1),\n",
    "    \"lag12\": g.shift(12),\n",
    "    \"ma3\":   g.shift(1).rolling(3).mean(),\n",
    "    \"ma6\":   g.shift(1).rolling(6).mean(),\n",
    "}\n",
    "\n",
    "#評估\n",
    "rows = []\n",
    "y_true = y.to_numpy(float)\n",
    "for name, p in preds.items():\n",
    "    y_pred = np.where(np.isnan(p), 0.0, p)  # 前幾期沒有歷史 → 補0\n",
    "    y_pred = np.clip(y_pred, 0.0, None)     # y≥0\n",
    "    score  = two_stage_mape_score(y_true, y_pred)\n",
    "    ratio  = frac_big_ape(y_true, y_pred)\n",
    "    rows.append((name, score, ratio))\n",
    "\n",
    "print(pd.DataFrame(rows, columns=[\"baseline\",\"two_stage_score\",\"frac_APE>100%\"])\n",
    "        .sort_values(\"two_stage_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af8d569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear:CV (Kaggle two-stage score) mean: 0.8912141569386278\n",
      "Linear:Each split: [0.95593319 0.91629137 0.88740826 0.829872   0.86656597]\n",
      "RF:CV (Kaggle two-stage score) mean: 0.2979890502873241\n",
      "RF:Each split: [0.74058429 0.         0.74936097 0.         0.        ]\n",
      "XGB:CV (Kaggle two-stage score) mean: 0.7308548434610529\n",
      "XGB:Each split: [0.88271885 0.         0.93597849 0.90699291 0.92858396]\n"
     ]
    }
   ],
   "source": [
    "date_col  = \"year_month\"   \n",
    "group_col = \"sector\"\n",
    "\n",
    "\n",
    "#時間序列排序\n",
    "g = df.groupby(group_col)[\"amount_new_house_transactions\"]\n",
    "\n",
    "df['lag1'] = g.shift(1)\n",
    "df['lag3'] = g.shift(3)\n",
    "df['lag12'] = g.shift(12)\n",
    "df['ma3'] = g.shift(1).rolling(3).mean()\n",
    "df['ma6'] = g.shift(1).rolling(6).mean()\n",
    "\n",
    "m=pd.to_datetime(df['year_month']).dt.month\n",
    "df['month_sin']=np.sin(2*np.pi*m/12)\n",
    "df['month_cos']=np.cos(2*np.pi*m/12)\n",
    "\n",
    "#重新切分\n",
    "X=df.drop('amount_new_house_transactions',axis=1)\n",
    "y=df['amount_new_house_transactions']\n",
    "\n",
    "\n",
    "#刪除NaN值的X欄\n",
    "# 1. 全 NaN\n",
    "nan_cols = X.columns[X.isna().all()]\n",
    "\n",
    "# 2. 幾乎全 NaN（例如超過 95% 缺失）\n",
    "mostly_nan_cols = X.columns[X.isna().mean() > 0.95]\n",
    "\n",
    "# 3. 常數或接近常數欄\n",
    "const_cols = X.columns[X.nunique(dropna=True) <= 1]\n",
    "\n",
    "# 設定一個閾值，例如：非 NaN 比例 < 5% 就刪掉\n",
    "low_info_cols = X.columns[(X.notna().mean() < 0.05)]\n",
    "\n",
    "drop_cols = nan_cols.union(mostly_nan_cols).union(const_cols).union(low_info_cols)\n",
    "\n",
    "X=X.drop(columns=drop_cols)\n",
    "\n",
    "\n",
    "#切分欄位\n",
    "time_cols=['lag1','lag3','lag12','ma3','ma6','month_sin','month_cos']\n",
    "cat_cols=X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "num_cols=[c for c in X.columns if c not in time_cols+cat_cols]\n",
    "\n",
    "\n",
    "#建立baseline\n",
    "pre_linear=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy='median')),\n",
    "            ('scaler',StandardScaler())\n",
    "        ]),num_cols),\n",
    "        ('time',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "        ]),time_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pre_tree=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy='median'))\n",
    "        ]),num_cols),\n",
    "        ('time',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "        ]),time_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "linear=LinearRegression()\n",
    "pipe_linear=Pipeline([\n",
    "    ('pre_linear',pre_linear),\n",
    "    ('linear',linear),\n",
    "])\n",
    "\n",
    "rf=RandomForestRegressor(\n",
    "    n_estimators=100,        \n",
    "    max_depth=12,            \n",
    "    min_samples_split=20,    \n",
    "    min_samples_leaf=5,      \n",
    "    max_features='sqrt',   \n",
    "    bootstrap=True,          \n",
    "    max_samples=0.7,         \n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "pipe_rf=Pipeline([\n",
    "    ('pre_rf',pre_tree),\n",
    "    ('rf',rf),\n",
    "])\n",
    "\n",
    "xgb=XGBRegressor(\n",
    "    n_estimators=200,        \n",
    "    max_depth=6,             \n",
    "    learning_rate=0.1,      \n",
    "    subsample=0.8,           \n",
    "    colsample_bytree=0.8,    \n",
    "    min_child_weight=5,      \n",
    "    reg_lambda=1.0,          \n",
    "    n_jobs=-1,              \n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"        \n",
    ")\n",
    "pipe_xgb=Pipeline([\n",
    "    ('pre_xgb',pre_tree),\n",
    "    ('xgb',xgb),\n",
    "])\n",
    "\n",
    "\n",
    "tscv=TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "cv_scores_linear=cross_val_score(\n",
    "    pipe_linear,X[num_cols+time_cols],y,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"Linear:CV (Kaggle two-stage score) mean:\", cv_scores_linear.mean())\n",
    "print(\"Linear:Each split:\", cv_scores_linear)\n",
    "\n",
    "cv_scores_rf=cross_val_score(\n",
    "    pipe_rf,X[num_cols+time_cols],y,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"RF:CV (Kaggle two-stage score) mean:\", cv_scores_rf.mean())\n",
    "print(\"RF:Each split:\", cv_scores_rf)\n",
    "\n",
    "cv_scores_xgb=cross_val_score(\n",
    "    pipe_xgb,X[num_cols+time_cols],y,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"XGB:CV (Kaggle two-stage score) mean:\", cv_scores_xgb.mean())\n",
    "print(\"XGB:Each split:\", cv_scores_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97480b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#刪除NaN值的X欄\n",
    "# 1. 全 NaN\n",
    "nan_cols = df.columns[df.isna().all()]\n",
    "\n",
    "# 2. 幾乎全 NaN（例如超過 95% 缺失）\n",
    "mostly_nan_cols = df.columns[df.isna().mean() > 0.95]\n",
    "\n",
    "# 3. 常數或接近常數欄\n",
    "const_cols = df.columns[df.nunique(dropna=True) <= 1]\n",
    "\n",
    "# 設定一個閾值，例如：非 NaN 比例 < 5% 就刪掉\n",
    "low_info_cols = df.columns[(df.notna().mean() < 0.05)]\n",
    "\n",
    "drop_cols = nan_cols.union(mostly_nan_cols).union(const_cols).union(low_info_cols)\n",
    "\n",
    "df=df.drop(columns=drop_cols)\n",
    "\n",
    "\n",
    "# === 策略 A：丟掉暖機段 ===\n",
    "def drop_warmup(df, group_col, date_col, max_lag=12):\n",
    "    df = df.sort_values([group_col, date_col]).copy()\n",
    "    pos = df.groupby(group_col).cumcount()\n",
    "    mask = pos >= max_lag\n",
    "    return df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# === 策略 B：補開頭 NaN ===\n",
    "def fill_warmup(df, group_col, lag_cols, method=\"bfill\"):\n",
    "    \"\"\"\n",
    "    補 lag/rolling 產生的 NaN（適合短序列）\n",
    "    method 可選: \"bfill\" 或 \"zero\"\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for c in lag_cols:\n",
    "        if method == \"bfill\":\n",
    "            df[c] = df.groupby(group_col)[c].bfill()\n",
    "        elif method == \"zero\":\n",
    "            df[c] = df[c].fillna(0.0)\n",
    "    return df\n",
    "\n",
    "\n",
    "group_col = \"sector\"\n",
    "date_col = \"year_month\"\n",
    "lag_cols = [\"lag1\",\"lag3\",\"lag12\",\"ma3\",\"ma6\"]\n",
    "\n",
    "# 丟掉暖機段\n",
    "df_A = drop_warmup(df, group_col=\"sector\", date_col=\"year_month\", max_lag=12)\n",
    "\n",
    "# 補開頭 NaN（用 bfill）\n",
    "df_B = fill_warmup(df, group_col=\"sector\", lag_cols=lag_cols, method=\"bfill\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38da9327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear:CV (Kaggle two-stage score) mean: 0.891644642268675\n",
      "Linear:Each split: [0.95617168 0.91935566 0.89337745 0.82402741 0.86529101]\n",
      "RF:CV (Kaggle two-stage score) mean: 0.30338740878552706\n",
      "RF:Each split: [0.76028943 0.         0.75664761 0.         0.        ]\n",
      "XGB:CV (Kaggle two-stage score) mean: 0.9014744529195398\n",
      "XGB:Each split: [0.88633493 0.85667574 0.93163905 0.90831517 0.92440737]\n"
     ]
    }
   ],
   "source": [
    "#重新切分\n",
    "X_A=df_A.drop('amount_new_house_transactions',axis=1)\n",
    "y_A=df_A['amount_new_house_transactions']\n",
    "\n",
    "#切分欄位\n",
    "time_cols=['lag1','lag3','lag12','ma3','ma6','month_sin','month_cos']\n",
    "cat_cols=X_A.select_dtypes(exclude=['number']).columns.tolist()\n",
    "num_cols=[c for c in X_A.columns if c not in time_cols+cat_cols]\n",
    "\n",
    "\n",
    "#建立baseline\n",
    "pre_linear=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy='median')),\n",
    "            ('scaler',StandardScaler())\n",
    "        ]),num_cols),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "pre_tree=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy='median'))\n",
    "        ]),num_cols),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "\n",
    "linear=LinearRegression()\n",
    "pipe_linear=Pipeline([\n",
    "    ('pre_linear',pre_linear),\n",
    "    ('linear',linear),\n",
    "])\n",
    "\n",
    "rf=RandomForestRegressor(\n",
    "    n_estimators=100,        \n",
    "    max_depth=12,            \n",
    "    min_samples_split=20,    \n",
    "    min_samples_leaf=5,      \n",
    "    max_features='sqrt',   \n",
    "    bootstrap=True,          \n",
    "    max_samples=0.7,         \n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "pipe_rf=Pipeline([\n",
    "    ('pre_rf',pre_tree),\n",
    "    ('rf',rf),\n",
    "])\n",
    "\n",
    "xgb=XGBRegressor(\n",
    "    n_estimators=200,        \n",
    "    max_depth=6,             \n",
    "    learning_rate=0.1,      \n",
    "    subsample=0.8,           \n",
    "    colsample_bytree=0.8,    \n",
    "    min_child_weight=5,      \n",
    "    reg_lambda=1.0,          \n",
    "    n_jobs=-1,              \n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"        \n",
    ")\n",
    "pipe_xgb=Pipeline([\n",
    "    ('pre_xgb',pre_tree),\n",
    "    ('xgb',xgb),\n",
    "])\n",
    "\n",
    "\n",
    "tscv=TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "cv_scores_linear=cross_val_score(\n",
    "    pipe_linear,X_A[num_cols+time_cols],y_A,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"Linear:CV (Kaggle two-stage score) mean:\", cv_scores_linear.mean())\n",
    "print(\"Linear:Each split:\", cv_scores_linear)\n",
    "\n",
    "cv_scores_rf=cross_val_score(\n",
    "    pipe_rf,X_A[num_cols+time_cols],y_A,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"RF:CV (Kaggle two-stage score) mean:\", cv_scores_rf.mean())\n",
    "print(\"RF:Each split:\", cv_scores_rf)\n",
    "\n",
    "cv_scores_xgb=cross_val_score(\n",
    "    pipe_xgb,X_A[num_cols+time_cols],y_A,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"XGB:CV (Kaggle two-stage score) mean:\", cv_scores_xgb.mean())\n",
    "print(\"XGB:Each split:\", cv_scores_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0de50c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear:CV (Kaggle two-stage score) mean: 0.8918158741038805\n",
      "Linear:Each split: [0.95625886 0.91927167 0.89287296 0.82472287 0.865953  ]\n",
      "RF:CV (Kaggle two-stage score) mean: 0.2944391667188054\n",
      "RF:Each split: [0.7546939  0.         0.71750194 0.         0.        ]\n",
      "XGB:CV (Kaggle two-stage score) mean: 0.7244165382045891\n",
      "XGB:Each split: [0.87638053 0.         0.92564204 0.89128615 0.92877396]\n"
     ]
    }
   ],
   "source": [
    "#重新切分\n",
    "X_B=df_B.drop('amount_new_house_transactions',axis=1)\n",
    "y_B=df_B['amount_new_house_transactions']\n",
    "\n",
    "#切分欄位\n",
    "time_cols=['lag1','lag3','lag12','ma3','ma6','month_sin','month_cos']\n",
    "cat_cols=X_B.select_dtypes(exclude=['number']).columns.tolist()\n",
    "num_cols=[c for c in X_B.columns if c not in time_cols+cat_cols]\n",
    "\n",
    "\n",
    "#建立baseline\n",
    "pre_linear=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy='median')),\n",
    "            ('scaler',StandardScaler())\n",
    "        ]),num_cols),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "pre_tree=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy='median'))\n",
    "        ]),num_cols),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "\n",
    "linear=LinearRegression()\n",
    "pipe_linear=Pipeline([\n",
    "    ('pre_linear',pre_linear),\n",
    "    ('linear',linear),\n",
    "])\n",
    "\n",
    "rf=RandomForestRegressor(\n",
    "    n_estimators=100,        \n",
    "    max_depth=12,            \n",
    "    min_samples_split=20,    \n",
    "    min_samples_leaf=5,      \n",
    "    max_features='sqrt',   \n",
    "    bootstrap=True,          \n",
    "    max_samples=0.7,         \n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "pipe_rf=Pipeline([\n",
    "    ('pre_rf',pre_tree),\n",
    "    ('rf',rf),\n",
    "])\n",
    "\n",
    "xgb=XGBRegressor(\n",
    "    n_estimators=200,        \n",
    "    max_depth=6,             \n",
    "    learning_rate=0.1,      \n",
    "    subsample=0.8,           \n",
    "    colsample_bytree=0.8,    \n",
    "    min_child_weight=5,      \n",
    "    reg_lambda=1.0,          \n",
    "    n_jobs=-1,              \n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"        \n",
    ")\n",
    "pipe_xgb=Pipeline([\n",
    "    ('pre_xgb',pre_tree),\n",
    "    ('xgb',xgb),\n",
    "])\n",
    "\n",
    "\n",
    "tscv=TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "cv_scores_linear=cross_val_score(\n",
    "    pipe_linear,X_B[num_cols+time_cols],y_B,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"Linear:CV (Kaggle two-stage score) mean:\", cv_scores_linear.mean())\n",
    "print(\"Linear:Each split:\", cv_scores_linear)\n",
    "\n",
    "cv_scores_rf=cross_val_score(\n",
    "    pipe_rf,X_B[num_cols+time_cols],y_B,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"RF:CV (Kaggle two-stage score) mean:\", cv_scores_rf.mean())\n",
    "print(\"RF:Each split:\", cv_scores_rf)\n",
    "\n",
    "cv_scores_xgb=cross_val_score(\n",
    "    pipe_xgb,X_B[num_cols+time_cols],y_B,cv=tscv,scoring=kaggle_scorer\n",
    ")\n",
    "print(\"XGB:CV (Kaggle two-stage score) mean:\", cv_scores_xgb.mean())\n",
    "print(\"XGB:Each split:\", cv_scores_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f72d2ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best CV score: 0.9313715950395031\n",
      "Best params: {'xgb_base__colsample_bytree': np.float64(0.7801997007878172), 'xgb_base__learning_rate': np.float64(0.030928547281190655), 'xgb_base__max_depth': 4, 'xgb_base__min_child_weight': 16, 'xgb_base__reg_lambda': np.float64(1.2575899703374167), 'xgb_base__subsample': np.float64(0.7541666010159664)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from scipy.stats import randint, uniform\n",
    "import numpy as np\n",
    "\n",
    "time_cols=['lag1','lag3','lag12','ma3','ma6','month_sin','month_cos']\n",
    "cat_cols=X_A.select_dtypes(exclude=['number']).columns.tolist()\n",
    "num_cols=[c for c in X_A.columns if c not in time_cols+cat_cols]\n",
    "\n",
    "# 基礎模型（穩定的缺省值）\n",
    "xgb_base = XGBRegressor(\n",
    "    n_estimators=900,           # 偏高，讓小學習率有空間\n",
    "    learning_rate=0.05,\n",
    "    objective=\"reg:squarederror\",\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=\"hist\",       \n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "pipe_xgb_ran=Pipeline([\n",
    "    ('pre_xgb',pre_tree),\n",
    "    ('xgb_base',xgb_base),\n",
    "])\n",
    "\n",
    "# 搜尋空間\n",
    "param_dist = {\n",
    "    \"xgb_base__max_depth\": randint(4, 7),\n",
    "    \"xgb_base__min_child_weight\": randint(5, 21),\n",
    "    \"xgb_base__subsample\": uniform(0.6, 0.4),\n",
    "    \"xgb_base__colsample_bytree\": uniform(0.6, 0.4),\n",
    "    \"xgb_base__reg_lambda\": uniform(0.3, 1.7),\n",
    "    \"xgb_base__learning_rate\": uniform(0.03, 0.07),\n",
    "}\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipe_xgb_ran,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=40,\n",
    "    cv=tscv,\n",
    "    scoring=kaggle_scorer,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "search.fit(X_A[num_cols+time_cols], y_A)\n",
    "\n",
    "print(\"Best CV score:\", search.best_score_)\n",
    "print(\"Best params:\", search.best_params_)\n",
    "\n",
    "best_xgb = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c166e659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重複組合數： 6365\n",
      "        sector year_month  cnt\n",
      "36    sector 1 2022-01-01  120\n",
      "37    sector 1 2022-02-01  120\n",
      "38    sector 1 2022-03-01  120\n",
      "39    sector 1 2022-04-01  120\n",
      "40    sector 1 2022-05-01  120\n",
      "41    sector 1 2022-06-01  120\n",
      "42    sector 1 2022-07-01  120\n",
      "43    sector 1 2022-08-01  120\n",
      "44    sector 1 2022-09-01  120\n",
      "45    sector 1 2022-10-01  120\n",
      "46    sector 1 2022-11-01  120\n",
      "47    sector 1 2022-12-01  120\n",
      "103  sector 10 2022-01-01  120\n",
      "104  sector 10 2022-02-01  120\n",
      "105  sector 10 2022-03-01  120\n",
      "106  sector 10 2022-04-01  120\n",
      "107  sector 10 2022-05-01  120\n",
      "108  sector 10 2022-06-01  120\n",
      "109  sector 10 2022-07-01  120\n",
      "110  sector 10 2022-08-01  120\n"
     ]
    }
   ],
   "source": [
    "date_col  = \"year_month\"   # 你的日期欄\n",
    "group_col = \"sector\"       # 群組欄\n",
    "y_col     = \"amount_new_house_transactions\"  # 目標欄（需用到時間特徵）\n",
    "\n",
    "df = df.copy()\n",
    "df[date_col] = pd.to_datetime(df[date_col]).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# 檢查欄位型別一致（常見問題：sector 有的 int、有的 str）\n",
    "df[group_col] = df[group_col].astype(str)\n",
    "\n",
    "# 哪些 key 重複？\n",
    "dups = (df.groupby([group_col, date_col]).size()\n",
    "          .reset_index(name=\"cnt\")\n",
    "          .query(\"cnt > 1\")\n",
    "          .sort_values([\"cnt\", group_col, date_col], ascending=[False, True, True]))\n",
    "\n",
    "print(\"重複組合數：\", len(dups))\n",
    "print(dups.head(20))          # 先看看前 20 筆\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8ce43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['month', 'sector', 'year', 'amount_new_house_transactions', 'year_end_registered_population_10k', 'total_households_10k', 'year_end_resident_population_10k', 'year_end_total_employed_population_10k', 'year_end_urban_non_private_employees_10k', 'private_individual_and_other_employees_10k', 'private_individual_ratio', 'national_year_end_total_population_10k', 'resident_registered_ratio', 'under_18_10k', '18_60_years_10k', 'over_60_years_10k', 'total', 'under_18_percent', '18_60_years_percent', 'over_60_years_percent', 'gdp_100m', 'primary_industry_100m', 'secondary_industry_100m', 'tertiary_industry_100m', 'gdp_per_capita_yuan', 'national_gdp_100m', 'national_economic_primacy', 'national_population_share', 'gdp_population_ratio', 'secondary_industry_development_gdp_share', 'tertiary_industry_development_gdp_share', 'employed_population', 'primary_industry_percent', 'secondary_industry_percent', 'tertiary_industry_percent', 'white_collar_service_vs_blue_collar_manufacturing_ratio', 'general_public_budget_revenue_100m', 'personal_income_tax_100m', 'per_capita_personal_income_tax_yuan', 'general_public_budget_expenditure_100m', 'total_retail_sales_of_consumer_goods_100m', 'retail_sales_growth_rate', 'urban_consumer_price_index_previous_year_100', 'annual_average_wage_urban_non_private_employees_yuan', 'annual_average_wage_urban_non_private_on_duty_employees_yuan', 'per_capita_disposable_income_absolute_yuan', 'per_capita_disposable_income_index_previous_year_100', 'engel_coefficient', 'per_capita_housing_area_sqm', 'number_of_universities', 'university_students_10k', 'number_of_middle_schools', 'middle_school_students_10k', 'number_of_primary_schools', 'primary_school_students_10k', 'number_of_kindergartens', 'kindergarten_students_10k', 'hospitals_health_centers', 'hospital_beds_10k', 'health_technical_personnel_10k', 'doctors_10k', 'road_length_km', 'road_area_10k_sqm', 'per_capita_urban_road_area_sqm', 'number_of_operating_bus_lines', 'operating_bus_line_length_km', 'internet_broadband_access_subscribers_10k', 'internet_broadband_access_ratio', 'number_of_industrial_enterprises_above_designated_size', 'total_current_assets_10k', 'science_expenditure_10k', 'education_expenditure_10k', 'keyword', 'source', 'search_volume', 'num_land_transactions', 'construction_area', 'planned_building_area', 'transaction_amount', 'num_land_transactions_nearby_sectors', 'construction_area_nearby_sectors', 'planned_building_area_nearby_sectors', 'transaction_amount_nearby_sectors', 'num_new_house_transactions_nearby_sectors', 'area_new_house_transactions_nearby_sectors', 'price_new_house_transactions_nearby_sectors', 'amount_new_house_transactions_nearby_sectors', 'area_per_unit_new_house_transactions_nearby_sectors', 'total_price_per_unit_new_house_transactions_nearby_sectors', 'num_new_house_available_for_sale_nearby_sectors', 'area_new_house_available_for_sale_nearby_sectors', 'period_new_house_sell_through_nearby_sectors', 'area_pre_owned_house_transactions', 'amount_pre_owned_house_transactions', 'num_pre_owned_house_transactions', 'price_pre_owned_house_transactions', 'num_pre_owned_house_transactions_nearby_sectors', 'area_pre_owned_house_transactions_nearby_sectors', 'amount_pre_owned_house_transactions_nearby_sectors', 'price_pre_owned_house_transactions_nearby_sectors', 'sector_coverage', 'population_scale', 'residential_area', 'office_building', 'commercial_area', 'resident_population', 'office_population', 'number_of_shops', 'catering', 'retail', 'hotel', 'transportation_station', 'education', 'leisure_and_entertainment', 'bus_station_cnt', 'subway_station_cnt', 'rentable_shops', 'surrounding_housing_average_price', 'surrounding_shop_average_rent', 'leisure_entertainment_entertainment_venue_game_arcade', 'leisure_entertainment_entertainment_venue_party_house', 'leisure_entertainment_cultural_venue_cultural_palace', 'education_training_school_education_middle_school', 'education_training_school_education_primary_school', 'education_training_school_education_kindergarten', 'education_training_school_education_research_institution', 'medical_health', 'medical_health_specialty_hospital', 'medical_health_tcm_hospital', 'medical_health_physical_examination_institution', 'medical_health_veterinary_station', 'medical_health_pharmaceutical_healthcare', 'medical_health_rehabilitation_institution', 'medical_health_first_aid_center', 'medical_health_blood_donation_station', 'medical_health_disease_prevention_institution', 'medical_health_general_hospital', 'medical_health_clinic', 'transportation_facilities_service_bus_station', 'transportation_facilities_service_subway_station', 'transportation_facilities_service_airport_related', 'transportation_facilities_service_port_terminal', 'transportation_facilities_service_train_station', 'transportation_facilities_service_long_distance_bus_station', 'number_of_leisure_and_entertainment_stores', 'number_of_other_stores', 'number_of_other_anchor_stores', 'number_of_home_appliance_stores', 'number_of_skincare_cosmetics_stores', 'number_of_fashion_stores', 'number_of_service_stores', 'number_of_jewelry_stores', 'number_of_lifestyle_leisure_stores', 'number_of_supermarket_convenience_stores', 'number_of_catering_food_stores', 'number_of_residential_commercial', 'number_of_office_building_commercial', 'number_of_commercial_buildings', 'number_of_hypermarkets', 'number_of_department_stores', 'number_of_shopping_centers', 'number_of_hotel_commercial', 'number_of_third_tier_shopping_malls_in_business_district', 'number_of_second_tier_shopping_malls_in_business_district', 'number_of_city_winner_malls', 'number_of_unranked_malls', 'number_of_community_malls', 'number_of_community_winner_malls', 'population_scale_dense', 'residential_area_dense', 'office_building_dense', 'commercial_area_dense', 'resident_population_dense', 'office_population_dense', 'number_of_shops_dense', 'catering_dense', 'retail_dense', 'hotel_dense', 'transportation_station_dense', 'education_dense', 'leisure_and_entertainment_dense', 'bus_station_cnt_dense', 'subway_station_cnt_dense', 'rentable_shops_dense', 'leisure_entertainment_stores_dense', 'other_stores_dense', 'other_anchor_stores_dense', 'home_appliance_stores_dense', 'skincare_cosmetics_stores_dense', 'fashion_stores_dense', 'service_stores_dense', 'jewelry_stores_dense', 'lifestyle_leisure_stores_dense', 'supermarket_convenience_stores_dense', 'catering_food_stores_dense', 'residential_commercial_dense', 'office_building_commercial_dense', 'commercial_buildings_dense', 'hypermarkets_dense', 'department_stores_dense', 'shopping_centers_dense', 'hotel_commercial_dense', 'third_tier_shopping_malls_in_business_district_dense', 'second_tier_shopping_malls_in_business_district_dense', 'city_winner_malls_dense', 'unranked_malls_dense', 'community_malls_dense', 'community_winner_malls_dense', 'transportation_facilities_service_bus_station_dense', 'transportation_facilities_service_subway_station_dense', 'transportation_facilities_service_airport_related_dense', 'transportation_facilities_service_port_terminal_dense', 'transportation_facilities_service_train_station_dense', 'transportation_facilities_service_long_distance_bus_station_dense', 'leisure_entertainment_entertainment_venue_game_arcade_dense', 'leisure_entertainment_entertainment_venue_party_house_dense', 'leisure_entertainment_cultural_venue_cultural_palace_dense', 'medical_health_dense', 'medical_health_specialty_hospital_dense', 'medical_health_tcm_hospital_dense', 'medical_health_physical_examination_institution_dense', 'medical_health_veterinary_station_dense', 'medical_health_pharmaceutical_healthcare_dense', 'medical_health_rehabilitation_institution_dense', 'medical_health_first_aid_center_dense', 'medical_health_blood_donation_station_dense', 'medical_health_disease_prevention_institution_dense', 'medical_health_general_hospital_dense', 'medical_health_clinic_dense', 'education_training_school_education_middle_school_dense', 'education_training_school_education_primary_school_dense', 'education_training_school_education_kindergarten_dense', 'education_training_school_education_research_institution_dense', 'year_month', 'lag1', 'lag3', 'lag12', 'ma3', 'ma6', 'month_sin', 'month_cos']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb1d265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_time_features_and_test_v5(\n",
    "    df: pd.DataFrame,\n",
    "    test_csv_path: str = \"test/test.csv\",\n",
    "    sample_csv_path: str = \"sample_submission.csv\",\n",
    "    date_col: str = \"year_month\",\n",
    "    group_col: str = \"sector\",\n",
    "    y_col: str = \"amount_new_house_transactions\",\n",
    "    use_month_start: bool = True,\n",
    "    use_ffill_for_y: bool = True,     # ← 新增：只用於產生時間特徵\n",
    "    sum_whitelist: list | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    v5: 統一 sector 為數字字串；每個 sector 補到 test 月份；可選擇用 ffill 的 y 來計算 lag/ma。\n",
    "    回傳: feat_df, X_test_features, time_cols, sample_ids, test_id_series\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # A) sector 統一為純數字字串\n",
    "    df[group_col] = (\n",
    "        df[group_col].astype(str)\n",
    "                      .str.replace(r\"^sector\\s*\", \"\", regex=True)\n",
    "                      .str.strip()\n",
    "    )\n",
    "\n",
    "    # B) 月粒度統一\n",
    "    if use_month_start:\n",
    "        df[date_col] = pd.to_datetime(df[date_col]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "        _freq = \"MS\"\n",
    "    else:\n",
    "        df[date_col] = pd.to_datetime(df[date_col]).dt.to_period(\"M\").dt.to_timestamp(how=\"end\")\n",
    "        _freq = \"M\"\n",
    "\n",
    "    # C) 聚合字典（預設 mean，流量欄 sum）\n",
    "    num_cols_all = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    agg_dict = {c: \"mean\" for c in num_cols_all}\n",
    "    default_sum_cols = [\n",
    "        \"num_land_transactions\",\"construction_area\",\"planned_building_area\",\"transaction_amount\",\n",
    "        \"num_land_transactions_nearby_sectors\",\"construction_area_nearby_sectors\",\n",
    "        \"planned_building_area_nearby_sectors\",\"transaction_amount_nearby_sectors\",\n",
    "        \"num_new_house_transactions\",\"area_new_house_transactions\",\"amount_new_house_transactions\",\n",
    "        \"num_new_house_transactions_nearby_sectors\",\"area_new_house_transactions_nearby_sectors\",\n",
    "        \"amount_new_house_transactions_nearby_sectors\",\n",
    "        \"num_pre_owned_house_transactions\",\"area_pre_owned_house_transactions\",\"amount_pre_owned_house_transactions\",\n",
    "        \"num_pre_owned_house_transactions_nearby_sectors\",\"area_pre_owned_house_transactions_nearby_sectors\",\n",
    "        \"amount_pre_owned_house_transactions_nearby_sectors\",\n",
    "    ]\n",
    "    sum_whitelist = default_sum_cols if sum_whitelist is None else sum_whitelist\n",
    "    pattern = re.compile(r\"(transaction|transactions).*(num|area|amount)\")\n",
    "    for c in df.columns:\n",
    "        if c in num_cols_all and pattern.search(c):\n",
    "            agg_dict[c] = \"sum\"\n",
    "    for c in sum_whitelist:\n",
    "        if c in num_cols_all:\n",
    "            agg_dict[c] = \"sum\"\n",
    "    agg_dict = {k: v for k, v in agg_dict.items() if k in num_cols_all}\n",
    "\n",
    "    df_agg = df.groupby([group_col, date_col], as_index=False).agg(agg_dict)\n",
    "\n",
    "    # D) 解析 test → (date, sector)\n",
    "    test = pd.read_csv(test_csv_path)\n",
    "    tmp = test[\"id\"].str.extract(r\"^(?P<year>\\d{4})\\s+(?P<mon>[A-Za-z]{3})_sector\\s+(?P<sector>\\d+)$\")\n",
    "    tmp[date_col]  = pd.to_datetime(tmp[\"year\"] + \" \" + tmp[\"mon\"] + \" 01\")\n",
    "    if not use_month_start:\n",
    "        tmp[date_col] = tmp[date_col].dt.to_period(\"M\").dt.to_timestamp(how=\"end\")\n",
    "    tmp[group_col] = tmp[\"sector\"].astype(str).str.strip()\n",
    "    target_end = tmp.groupby(group_col)[date_col].max().to_dict()\n",
    "\n",
    "    # E) 每個 sector 補到 test 的月份\n",
    "    pieces = []\n",
    "    grp = df_agg[[group_col, date_col, y_col]].groupby(group_col)\n",
    "    for key, g in grp:\n",
    "        g2 = g.set_index(date_col).sort_index()\n",
    "        start    = g2.index.min()\n",
    "        end_hist = g2.index.max()\n",
    "        end_test = target_end.get(str(key), end_hist)\n",
    "        end      = max(end_hist, end_test)\n",
    "        full_idx = pd.date_range(start=start, end=end, freq=_freq)\n",
    "        g2 = g2.reindex(full_idx)\n",
    "        g2.index.name = date_col\n",
    "        g2[group_col] = str(key)\n",
    "        pieces.append(g2.reset_index())\n",
    "    hist = pd.concat(pieces, ignore_index=True)\n",
    "\n",
    "    # F) 產生時間特徵（可選 ffill）\n",
    "    if use_ffill_for_y:\n",
    "        hist[\"y_ffill\"] = hist.groupby(group_col)[y_col].ffill()\n",
    "        base = hist.groupby(group_col)[\"y_ffill\"]\n",
    "    else:\n",
    "        base = hist.groupby(group_col)[y_col]\n",
    "\n",
    "    hist[\"lag1\"]  = base.shift(1)\n",
    "    hist[\"lag3\"]  = base.shift(3)\n",
    "    hist[\"lag12\"] = base.shift(12)\n",
    "    hist[\"ma3\"]   = base.shift(1).rolling(3).mean()\n",
    "    hist[\"ma6\"]   = base.shift(1).rolling(6).mean()\n",
    "\n",
    "    m = hist[date_col].dt.month\n",
    "    hist[\"month_sin\"] = np.sin(2*np.pi*m/12)\n",
    "    hist[\"month_cos\"] = np.cos(2*np.pi*m/12)\n",
    "\n",
    "    time_cols = [\"lag1\",\"lag3\",\"lag12\",\"ma3\",\"ma6\",\"month_sin\",\"month_cos\"]\n",
    "    feat_df   = hist[[date_col, group_col] + time_cols].copy()\n",
    "\n",
    "    # G) 左接到 test\n",
    "    X_test_idx = tmp[[date_col, group_col]].copy()\n",
    "    feat_df[group_col]  = feat_df[group_col].astype(str)\n",
    "    X_test_features = X_test_idx.merge(feat_df, on=[date_col, group_col], how=\"left\")\n",
    "\n",
    "    sample_ids = pd.read_csv(sample_csv_path)[\"id\"].tolist()\n",
    "    return feat_df, X_test_features, time_cols, sample_ids, test[\"id\"]\n",
    "\n",
    "\n",
    "\n",
    "def make_submission_from_best_v2(\n",
    "    best_xgb,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test_features: pd.DataFrame,\n",
    "    train_features: list,\n",
    "    sample_ids: list,\n",
    "    out_path: str = \"submission.csv\",\n",
    "    id_series: pd.Series | None = None,\n",
    "):\n",
    "    # 補齊測試欄\n",
    "    miss_train = [c for c in train_features if c not in X_train.columns]\n",
    "    if miss_train:\n",
    "        raise ValueError(f\"X_train 缺少欄位：{miss_train}\")\n",
    "    for c in train_features:\n",
    "        if c not in X_test_features.columns:\n",
    "            X_test_features[c] = np.nan\n",
    "\n",
    "    Xtr = X_train[train_features]\n",
    "    Xte = X_test_features[train_features]\n",
    "\n",
    "    best_xgb.fit(Xtr, y_train)\n",
    "    pred = best_xgb.predict(Xte)\n",
    "    pred = np.clip(pred, 0.0, None)\n",
    "\n",
    "    if id_series is None:\n",
    "        sub = pd.DataFrame({\"id\": sample_ids, \"new_house_transactions\": pred})\n",
    "    else:\n",
    "        sub = pd.DataFrame({\"id\": id_series, \"new_house_transactions\": pred})\n",
    "    sub = sub.set_index(\"id\").reindex(sample_ids).reset_index()\n",
    "    sub.to_csv(out_path, index=False)\n",
    "    print(f\"✅ Saved {out_path}, shape={sub.shape}\")\n",
    "    return sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lag1     0.01\n",
      "lag3     0.01\n",
      "lag12    0.01\n",
      "ma3      0.01\n",
      "ma6      0.01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "feat_df, X_test_features, time_cols, sample_ids, test_ids = build_time_features_and_test_v5(\n",
    "    df=df,\n",
    "    test_csv_path=\"test/test.csv\",\n",
    "    sample_csv_path=\"test/sample_submission.csv\",\n",
    "    date_col=\"year_month\",\n",
    "    group_col=\"sector\",\n",
    "    y_col=\"amount_new_house_transactions\",\n",
    "    use_month_start=True,\n",
    "    use_ffill_for_y=True,  # ← 開啟：先前向填補，再算 lag/ma\n",
    ")\n",
    "\n",
    "# 檢查 NaN 比例（應該顯著下降）\n",
    "print(X_test_features[[\"lag1\",\"lag3\",\"lag12\",\"ma3\",\"ma6\"]].isna().mean().round(3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "43a028be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lag1', 'lag3', 'lag12', 'ma3', 'ma6', 'month_sin', 'month_cos'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.feature_names_in_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "de7d27e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 7)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b823ecf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('num',\n",
       "  Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))]),\n",
       "  ['year_end_registered_population_10k',\n",
       "   'total_households_10k',\n",
       "   'year_end_resident_population_10k',\n",
       "   'year_end_total_employed_population_10k',\n",
       "   'year_end_urban_non_private_employees_10k',\n",
       "   'private_individual_and_other_employees_10k',\n",
       "   'private_individual_ratio',\n",
       "   'national_year_end_total_population_10k',\n",
       "   'resident_registered_ratio',\n",
       "   'under_18_10k',\n",
       "   '18_60_years_10k',\n",
       "   'over_60_years_10k',\n",
       "   'total',\n",
       "   'under_18_percent',\n",
       "   '18_60_years_percent',\n",
       "   'over_60_years_percent',\n",
       "   'gdp_100m',\n",
       "   'primary_industry_100m',\n",
       "   'secondary_industry_100m',\n",
       "   'tertiary_industry_100m',\n",
       "   'gdp_per_capita_yuan',\n",
       "   'national_gdp_100m',\n",
       "   'national_economic_primacy',\n",
       "   'national_population_share',\n",
       "   'gdp_population_ratio',\n",
       "   'secondary_industry_development_gdp_share',\n",
       "   'tertiary_industry_development_gdp_share',\n",
       "   'employed_population',\n",
       "   'primary_industry_percent',\n",
       "   'secondary_industry_percent',\n",
       "   'tertiary_industry_percent',\n",
       "   'white_collar_service_vs_blue_collar_manufacturing_ratio',\n",
       "   'general_public_budget_revenue_100m',\n",
       "   'personal_income_tax_100m',\n",
       "   'per_capita_personal_income_tax_yuan',\n",
       "   'general_public_budget_expenditure_100m',\n",
       "   'total_retail_sales_of_consumer_goods_100m',\n",
       "   'retail_sales_growth_rate',\n",
       "   'urban_consumer_price_index_previous_year_100',\n",
       "   'annual_average_wage_urban_non_private_employees_yuan',\n",
       "   'annual_average_wage_urban_non_private_on_duty_employees_yuan',\n",
       "   'per_capita_disposable_income_absolute_yuan',\n",
       "   'per_capita_disposable_income_index_previous_year_100',\n",
       "   'engel_coefficient',\n",
       "   'per_capita_housing_area_sqm',\n",
       "   'number_of_universities',\n",
       "   'university_students_10k',\n",
       "   'number_of_middle_schools',\n",
       "   'middle_school_students_10k',\n",
       "   'number_of_primary_schools',\n",
       "   'primary_school_students_10k',\n",
       "   'number_of_kindergartens',\n",
       "   'kindergarten_students_10k',\n",
       "   'hospitals_health_centers',\n",
       "   'hospital_beds_10k',\n",
       "   'health_technical_personnel_10k',\n",
       "   'doctors_10k',\n",
       "   'road_length_km',\n",
       "   'road_area_10k_sqm',\n",
       "   'per_capita_urban_road_area_sqm',\n",
       "   'number_of_operating_bus_lines',\n",
       "   'operating_bus_line_length_km',\n",
       "   'internet_broadband_access_subscribers_10k',\n",
       "   'internet_broadband_access_ratio',\n",
       "   'number_of_industrial_enterprises_above_designated_size',\n",
       "   'total_current_assets_10k',\n",
       "   'science_expenditure_10k',\n",
       "   'education_expenditure_10k',\n",
       "   'search_volume',\n",
       "   'num_land_transactions',\n",
       "   'construction_area',\n",
       "   'planned_building_area',\n",
       "   'transaction_amount',\n",
       "   'num_land_transactions_nearby_sectors',\n",
       "   'construction_area_nearby_sectors',\n",
       "   'planned_building_area_nearby_sectors',\n",
       "   'transaction_amount_nearby_sectors',\n",
       "   'num_new_house_transactions_nearby_sectors',\n",
       "   'area_new_house_transactions_nearby_sectors',\n",
       "   'price_new_house_transactions_nearby_sectors',\n",
       "   'amount_new_house_transactions_nearby_sectors',\n",
       "   'area_per_unit_new_house_transactions_nearby_sectors',\n",
       "   'total_price_per_unit_new_house_transactions_nearby_sectors',\n",
       "   'num_new_house_available_for_sale_nearby_sectors',\n",
       "   'area_new_house_available_for_sale_nearby_sectors',\n",
       "   'period_new_house_sell_through_nearby_sectors',\n",
       "   'area_pre_owned_house_transactions',\n",
       "   'amount_pre_owned_house_transactions',\n",
       "   'num_pre_owned_house_transactions',\n",
       "   'price_pre_owned_house_transactions',\n",
       "   'num_pre_owned_house_transactions_nearby_sectors',\n",
       "   'area_pre_owned_house_transactions_nearby_sectors',\n",
       "   'amount_pre_owned_house_transactions_nearby_sectors',\n",
       "   'price_pre_owned_house_transactions_nearby_sectors',\n",
       "   'sector_coverage',\n",
       "   'population_scale',\n",
       "   'residential_area',\n",
       "   'office_building',\n",
       "   'commercial_area',\n",
       "   'resident_population',\n",
       "   'office_population',\n",
       "   'number_of_shops',\n",
       "   'catering',\n",
       "   'retail',\n",
       "   'hotel',\n",
       "   'transportation_station',\n",
       "   'education',\n",
       "   'leisure_and_entertainment',\n",
       "   'bus_station_cnt',\n",
       "   'subway_station_cnt',\n",
       "   'rentable_shops',\n",
       "   'surrounding_housing_average_price',\n",
       "   'surrounding_shop_average_rent',\n",
       "   'leisure_entertainment_entertainment_venue_game_arcade',\n",
       "   'leisure_entertainment_entertainment_venue_party_house',\n",
       "   'leisure_entertainment_cultural_venue_cultural_palace',\n",
       "   'education_training_school_education_middle_school',\n",
       "   'education_training_school_education_primary_school',\n",
       "   'education_training_school_education_kindergarten',\n",
       "   'education_training_school_education_research_institution',\n",
       "   'medical_health',\n",
       "   'medical_health_specialty_hospital',\n",
       "   'medical_health_tcm_hospital',\n",
       "   'medical_health_physical_examination_institution',\n",
       "   'medical_health_veterinary_station',\n",
       "   'medical_health_pharmaceutical_healthcare',\n",
       "   'medical_health_rehabilitation_institution',\n",
       "   'medical_health_first_aid_center',\n",
       "   'medical_health_blood_donation_station',\n",
       "   'medical_health_disease_prevention_institution',\n",
       "   'medical_health_general_hospital',\n",
       "   'medical_health_clinic',\n",
       "   'transportation_facilities_service_bus_station',\n",
       "   'transportation_facilities_service_subway_station',\n",
       "   'transportation_facilities_service_airport_related',\n",
       "   'transportation_facilities_service_port_terminal',\n",
       "   'transportation_facilities_service_train_station',\n",
       "   'transportation_facilities_service_long_distance_bus_station',\n",
       "   'number_of_leisure_and_entertainment_stores',\n",
       "   'number_of_other_stores',\n",
       "   'number_of_other_anchor_stores',\n",
       "   'number_of_home_appliance_stores',\n",
       "   'number_of_skincare_cosmetics_stores',\n",
       "   'number_of_fashion_stores',\n",
       "   'number_of_service_stores',\n",
       "   'number_of_jewelry_stores',\n",
       "   'number_of_lifestyle_leisure_stores',\n",
       "   'number_of_supermarket_convenience_stores',\n",
       "   'number_of_catering_food_stores',\n",
       "   'number_of_residential_commercial',\n",
       "   'number_of_office_building_commercial',\n",
       "   'number_of_commercial_buildings',\n",
       "   'number_of_hypermarkets',\n",
       "   'number_of_department_stores',\n",
       "   'number_of_shopping_centers',\n",
       "   'number_of_hotel_commercial',\n",
       "   'number_of_third_tier_shopping_malls_in_business_district',\n",
       "   'number_of_second_tier_shopping_malls_in_business_district',\n",
       "   'number_of_city_winner_malls',\n",
       "   'number_of_unranked_malls',\n",
       "   'number_of_community_malls',\n",
       "   'number_of_community_winner_malls',\n",
       "   'population_scale_dense',\n",
       "   'residential_area_dense',\n",
       "   'office_building_dense',\n",
       "   'commercial_area_dense',\n",
       "   'resident_population_dense',\n",
       "   'office_population_dense',\n",
       "   'number_of_shops_dense',\n",
       "   'catering_dense',\n",
       "   'retail_dense',\n",
       "   'hotel_dense',\n",
       "   'transportation_station_dense',\n",
       "   'education_dense',\n",
       "   'leisure_and_entertainment_dense',\n",
       "   'bus_station_cnt_dense',\n",
       "   'subway_station_cnt_dense',\n",
       "   'rentable_shops_dense',\n",
       "   'leisure_entertainment_stores_dense',\n",
       "   'other_stores_dense',\n",
       "   'other_anchor_stores_dense',\n",
       "   'home_appliance_stores_dense',\n",
       "   'skincare_cosmetics_stores_dense',\n",
       "   'fashion_stores_dense',\n",
       "   'service_stores_dense',\n",
       "   'jewelry_stores_dense',\n",
       "   'lifestyle_leisure_stores_dense',\n",
       "   'supermarket_convenience_stores_dense',\n",
       "   'catering_food_stores_dense',\n",
       "   'residential_commercial_dense',\n",
       "   'office_building_commercial_dense',\n",
       "   'commercial_buildings_dense',\n",
       "   'hypermarkets_dense',\n",
       "   'department_stores_dense',\n",
       "   'shopping_centers_dense',\n",
       "   'hotel_commercial_dense',\n",
       "   'third_tier_shopping_malls_in_business_district_dense',\n",
       "   'second_tier_shopping_malls_in_business_district_dense',\n",
       "   'city_winner_malls_dense',\n",
       "   'unranked_malls_dense',\n",
       "   'community_malls_dense',\n",
       "   'community_winner_malls_dense',\n",
       "   'transportation_facilities_service_bus_station_dense',\n",
       "   'transportation_facilities_service_subway_station_dense',\n",
       "   'transportation_facilities_service_airport_related_dense',\n",
       "   'transportation_facilities_service_port_terminal_dense',\n",
       "   'transportation_facilities_service_train_station_dense',\n",
       "   'transportation_facilities_service_long_distance_bus_station_dense',\n",
       "   'leisure_entertainment_entertainment_venue_game_arcade_dense',\n",
       "   'leisure_entertainment_entertainment_venue_party_house_dense',\n",
       "   'leisure_entertainment_cultural_venue_cultural_palace_dense',\n",
       "   'medical_health_dense',\n",
       "   'medical_health_specialty_hospital_dense',\n",
       "   'medical_health_tcm_hospital_dense',\n",
       "   'medical_health_physical_examination_institution_dense',\n",
       "   'medical_health_veterinary_station_dense',\n",
       "   'medical_health_pharmaceutical_healthcare_dense',\n",
       "   'medical_health_rehabilitation_institution_dense',\n",
       "   'medical_health_first_aid_center_dense',\n",
       "   'medical_health_blood_donation_station_dense',\n",
       "   'medical_health_disease_prevention_institution_dense',\n",
       "   'medical_health_general_hospital_dense',\n",
       "   'medical_health_clinic_dense',\n",
       "   'education_training_school_education_middle_school_dense',\n",
       "   'education_training_school_education_primary_school_dense',\n",
       "   'education_training_school_education_kindergarten_dense',\n",
       "   'education_training_school_education_research_institution_dense']),\n",
       " ('remainder',\n",
       "  FunctionTransformer(accept_sparse=True, check_inverse=False,\n",
       "                      feature_names_out='one-to-one'),\n",
       "  [227, 228, 229, 230, 231, 232])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c5acf366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已輸出 submission.csv，shape = (1152, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1) 取出你 RandomizedSearchCV 找到的 XGB 參數（或手動寫死）\n",
    "xgb_step_name = None\n",
    "for k in best_xgb.named_steps:\n",
    "    if isinstance(best_xgb.named_steps[k], XGBRegressor):\n",
    "        xgb_step_name = k\n",
    "        break\n",
    "\n",
    "if xgb_step_name is None:\n",
    "    raise RuntimeError(\"找不到 XGBRegressor 步驟，請確認 best_xgb 的步驟名稱\")\n",
    "\n",
    "best_params = best_xgb.named_steps[xgb_step_name].get_params()\n",
    "# 可選：刪除不必要/衝突參數\n",
    "best_params.pop('n_jobs', None)\n",
    "best_params.pop('random_state', None)\n",
    "\n",
    "# 2) 只用時間特徵的前處理 + XGB\n",
    "pre_time_only = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"time\", SimpleImputer(strategy=\"constant\", fill_value=0.0), time_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "pipe_time_only = Pipeline([\n",
    "    (\"pre\", pre_time_only),\n",
    "    (\"xgb\", XGBRegressor(**best_params))\n",
    "])\n",
    "\n",
    "# 3) 用訓練資料只選 time_cols 重新 fit（很快）\n",
    "X_train_time = X_A[time_cols]\n",
    "pipe_time_only.fit(X_train_time, y_A)\n",
    "\n",
    "# 4) 用測試資料的 time_cols 預測\n",
    "X_test_time = X_test_features[time_cols]\n",
    "y_test_pred = pipe_time_only.predict(X_test_time)\n",
    "y_test_pred = np.clip(y_test_pred, 0.0, None)\n",
    "\n",
    "# 5) 輸出提交檔（用 test.csv 的 id 順序）\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"new_house_transactions\": y_test_pred\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ 已輸出 submission.csv，shape =\", submission.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61787c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
